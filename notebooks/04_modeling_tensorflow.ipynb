# Train / evaluate / save

df = load_features()
X, y_raw = select_features_and_target(df)
(Xtr, ytr), (Xva, yva), le = preprocess(X, y_raw)

model = build_model(input_dim=Xtr.shape[1], num_classes=len(le.classes_))
callbacks = [
    keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_accuracy')
]
history = model.fit(
    Xtr, ytr,
    validation_data=(Xva, yva),
    epochs=30,
    batch_size=256,
    callbacks=callbacks,
    verbose=1
)
val_loss, val_acc = model.evaluate(Xva, yva, verbose=0)
print('Val accuracy:', val_acc)

y_pred = model.predict(Xva, verbose=0).argmax(axis=1)
print('Macro F1:', f1_score(yva, y_pred, average='macro'))
print(classification_report(yva, y_pred, target_names=le.classes_.astype(str)))

model.save(MODELS_DIR / 'keras_ffn')def build_model(input_dim: int, num_classes: int, hidden_dims=(256, 128), dropout: float = 0.2):
    inputs = keras.Input(shape=(input_dim,), name='features')
    x = inputs
    for h in hidden_dims:
        x = layers.Dense(h, activation='relu')(x)
        x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    model = keras.Model(inputs, outputs, name='ffn_classifier')
    model.compile(
        optimizer=keras.optimizers.Adam(1e-3),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return modeldef load_features():
    if FEATURES_PATH.exists():
        return pd.read_parquet(FEATURES_PATH)
    raise FileNotFoundError('features.parquet not found. Run 02_feature_engineering.')


def select_features_and_target(df: pd.DataFrame):
    cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if TARGET_COLUMN not in df.columns:
        raise KeyError(f'Missing target: {TARGET_COLUMN}')
    X = df[cols].fillna(0.0).astype(np.float32)
    y_raw = df[TARGET_COLUMN]
    return X, y_raw


def preprocess(X, y_raw, test_size=0.2, random_state=42):
    le = LabelEncoder()
    y = le.fit_transform(y_raw.values).astype(np.int32)
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    joblib.dump(le, DATA_DIR / 'label_encoder.pkl')
    joblib.dump(scaler, DATA_DIR / 'scaler.pkl')
    return (X_train, y_train), (X_val, y_val), le# Imports and config
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, f1_score
import joblib
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

DATA_DIR = Path('../data').resolve()
FEATURES_PATH = DATA_DIR / 'features.parquet'
MODELS_DIR = Path('../models').resolve()
MODELS_DIR.mkdir(parents=True, exist_ok=True)

TARGET_COLUMN = 'disaster_type'  # Update to your actual target column# 04 â€” Deep Learning Modeling (TensorFlow/Keras)

Construct a Keras model with preprocessing layers. Train, validate, and compare to PyTorch and baseline models.