# Run training

df = load_features()
X, y_raw = select_features_and_target(df)
(Xtr, ytr), (Xva, yva), le = preprocess(X, y_raw)

train_ds = TensorDataset(torch.tensor(Xtr, dtype=torch.float32), torch.tensor(ytr, dtype=torch.long))
val_ds = TensorDataset(torch.tensor(Xva, dtype=torch.float32), torch.tensor(yva, dtype=torch.long))
train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=256)

model = FeedforwardNet(input_dim=Xtr.shape[1], num_classes=len(le.classes_))
device = 'cuda' if torch.cuda.is_available() else 'cpu'
train_model(model, train_loader, val_loader, epochs=20, lr=1e-3, device=device)

# Final evaluation report
model.load_state_dict(torch.load(MODELS_DIR / 'pytorch_ffn.pt', map_location=device))
model.eval()
y_true, y_pred = [], []
with torch.no_grad():
    for xb, yb in val_loader:
        logits = model(xb.to(device))
        y_pred.extend(logits.argmax(dim=1).cpu().numpy())
        y_true.extend(yb.numpy())
print(classification_report(y_true, y_pred, target_names=le.classes_.astype(str)))def train_model(model: nn.Module, train_loader, val_loader, epochs: int = 20, lr: float = 1e-3, device: str = 'cpu'):
    model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    best_val_acc = 0.0
    for epoch in range(1, epochs + 1):
        model.train()
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            logits = model(xb)
            loss = criterion(logits, yb)
            loss.backward()
            optimizer.step()
        # Eval
        model.eval()
        y_true, y_pred = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb = xb.to(device)
                logits = model(xb)
                preds = logits.argmax(dim=1).cpu().numpy()
                y_pred.extend(preds)
                y_true.extend(yb.numpy())
        acc = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred, average='macro')
        print(f'Epoch {epoch:03d} | val_acc={acc:.3f} val_macro_f1={f1:.3f}')
        if acc > best_val_acc:
            best_val_acc = acc
            torch.save(model.state_dict(), MODELS_DIR / 'pytorch_ffn.pt')
    print('Best val acc:', best_val_acc)class FeedforwardNet(nn.Module):
    def __init__(self, input_dim: int, num_classes: int, hidden_dims=(256, 128), dropout: float = 0.2):
        super().__init__()
        layers = []
        prev = input_dim
        for h in hidden_dims:
            layers += [nn.Linear(prev, h), nn.ReLU(), nn.Dropout(dropout)]
            prev = h
        layers += [nn.Linear(prev, num_classes)]
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)def load_features():
    if FEATURES_PATH.exists():
        return pd.read_parquet(FEATURES_PATH)
    raise FileNotFoundError('features.parquet not found. Run 02_feature_engineering.')


def select_features_and_target(df: pd.DataFrame):
    cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if TARGET_COLUMN in df.columns:
        y_raw = df[TARGET_COLUMN]
    else:
        raise KeyError(f'Missing target column: {TARGET_COLUMN}')
    X = df[cols].fillna(0.0).astype(np.float32)
    return X, y_raw


def preprocess(X, y_raw, test_size=0.2, random_state=42):
    le = LabelEncoder()
    y = le.fit_transform(y_raw.values).astype(np.int64)
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    joblib.dump(le, DATA_DIR / 'label_encoder.pkl')
    joblib.dump(scaler, DATA_DIR / 'scaler.pkl')
    return (X_train_scaled, y_train), (X_val_scaled, y_val), le# Imports and config
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, classification_report
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import joblib

DATA_DIR = Path('../data').resolve()
FEATURES_PATH = DATA_DIR / 'features.parquet'
MODELS_DIR = Path('../models').resolve()
MODELS_DIR.mkdir(parents=True, exist_ok=True)

TARGET_COLUMN = 'disaster_type'  # Update to your actual target column# 03 â€” Deep Learning Modeling (PyTorch)

Build and train a feedforward neural network on the processed features. Evaluate with accuracy, macro-F1, and ROC-AUC.