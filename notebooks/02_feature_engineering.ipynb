# Sanity checks
if FEATURES_PATH.exists():
    df = pd.read_parquet(FEATURES_PATH)
    print(df.shape)
    display(df.sample(min(5, len(df))))# Build features
try:
    base = load_base_data()
    ext = load_external_sources()
    feats = build_feature_table(base, ext)
    feats = merge_on_country_year(feats, ext)
    print('Feature shape:', feats.shape)
    display(feats.head(3))
    finalize_and_save(feats)
except Exception as e:
    print('Error:', e)# Helper functions

def load_base_data():
    if CLEAN_PATH.exists():
        return pd.read_parquet(CLEAN_PATH)
    elif RAW_PATH.exists():
        return pd.read_csv(RAW_PATH)
    else:
        raise FileNotFoundError('Place disasters.csv in data/ or run 01_data_exploration to produce disasters_processed.parquet')


def load_external_sources():
    # TODO: Replace with actual sources (NOAA, World Bank, etc.)
    # Placeholder keyed by country/year
    return pd.DataFrame(columns=['country', 'year'])


def standardize_geo_columns(df: pd.DataFrame) -> pd.DataFrame:
    lat_candidates = [c for c in df.columns if c.lower() in {'lat', 'latitude'}]
    lon_candidates = [c for c in df.columns if c.lower() in {'lon', 'longitude'}]
    if lat_candidates:
        df = df.rename(columns={lat_candidates[0]: 'latitude'})
    if lon_candidates:
        df = df.rename(columns={lon_candidates[0]: 'longitude'})
    return df


def build_feature_table(base_df: pd.DataFrame, external_df: pd.DataFrame) -> pd.DataFrame:
    df = standardize_geo_columns(base_df).copy()
    # Example engineered features
    if 'casualties' in df.columns:
        df['log_casualties'] = np.log1p(pd.to_numeric(df['casualties'], errors='coerce').fillna(0))
    if {'latitude', 'longitude'}.issubset(df.columns):
        df['abs_latitude'] = df['latitude'].abs()
    return df


def merge_on_country_year(df: pd.DataFrame, external_df: pd.DataFrame) -> pd.DataFrame:
    for c in ['country', 'Country']:
        if c in df.columns:
            df = df.rename(columns={c: 'country'})
            break
    for t in ['year', 'Year', 'YEAR']:
        if t in df.columns:
            df['year'] = pd.to_numeric(df[t], errors='coerce').astype('Int64')
            if t != 'year':
                df = df.drop(columns=[t])
            break
    merged = df.merge(external_df, on=['country', 'year'], how='left')
    return merged


def finalize_and_save(df: pd.DataFrame) -> None:
    FEATURES_PATH.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(FEATURES_PATH, index=False)
    # Save numeric feature column names to help risk mapping stage
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    (DATA_DIR / 'features_columns.txt').write_text('\n'.join(num_cols))
    print('Saved features to', FEATURES_PATH)
    print('Saved feature names to', DATA_DIR / 'features_columns.txt')# Imports and config
import pandas as pd
import numpy as np
from pathlib import Path

DATA_DIR = Path('../data').resolve()
RAW_PATH = DATA_DIR / 'disasters.csv'
CLEAN_PATH = DATA_DIR / 'disasters_processed.parquet'
FEATURES_PATH = DATA_DIR / 'features.parquet'

print('Data dir:', DATA_DIR)
print('Raw exists:', RAW_PATH.exists())
print('Clean exists:', CLEAN_PATH.exists())# 02 â€” Feature Engineering and External Data Merge

Create features from the base dataset, enrich with climate and socio-economic indicators, and persist a modeling-ready table.